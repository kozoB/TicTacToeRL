{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCyVLOcacknIFZiJI27m9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kozoB/TicTacToeRL/blob/main/TicTacToeRL_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ],
      "metadata": {
        "id": "9OzadXCWiEhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module Installations And Imports"
      ],
      "metadata": {
        "id": "ocZ1XaKhiVCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "knZVQZ08mhl9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Labels"
      ],
      "metadata": {
        "id": "_6uJsbduLXVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define state labels for tic tac toe game cells ('-' (empty), 'X' and 'O')\n",
        "mark_labels = ['-', 'X', 'O']\n",
        "# Define game result labels (0 - game ongoing, 1 - 'X' won, 2 - 'O' won, 3 - draw)\n",
        "game_result_labels = ['ongoing', 'X-won', 'O-won', 'draw']"
      ],
      "metadata": {
        "id": "RWQusHY81W9R"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Game Environment And Rules"
      ],
      "metadata": {
        "id": "vWvD3fz3LaAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToeEnv(Env):\n",
        "  def __init__(self):\n",
        "    # Actions we can take - Square in grid to mark (1-9)\n",
        "    self.action_space = Discrete(9)\n",
        "    # Observation space: 3x3 grid with 3 possible values (-, X, O) encoded as integers (0, 1, 2)\n",
        "    self.observation_space = Box(low=0, high=2, shape=(3, 3), dtype=np.int32)\n",
        "    # Initialize the game grid\n",
        "    self.game_grid = np.full((3, 3), 0, dtype=np.int32)\n",
        "    # Set starting player as the index of 'X' (1)\n",
        "    self.current_player = 1 # 1 for 'X', 2 for 'O'\n",
        "    # Episode status\n",
        "    self.done = False\n",
        "\n",
        "  def step(self, action):\n",
        "    # Convert action (0-8) to row and column indices (0-2)\n",
        "    row, col = divmod(action, 3)\n",
        "\n",
        "    # Check if the chosen sqaure is empty\n",
        "    if self.game_grid[row, col] != 0:\n",
        "      # Invalid action, return a large negative reward\n",
        "      return self.game_grid, -100, False, {}\n",
        "\n",
        "    # Mark the chosen square with the current player's mark\n",
        "    self.game_grid[row, col] = self.current_player\n",
        "\n",
        "    # Check the game result\n",
        "    game_result = self.check_game_result()\n",
        "\n",
        "    # Determine the reward based on the game result\n",
        "    if game_result == 1: # Assuming the agent is 'X'\n",
        "        reward = +1 if self.current_player == 1 else -1  # 'X' wins\n",
        "        self.done = True\n",
        "    elif game_result == 2:\n",
        "        reward = -1 if self.current_player == 1 else +1  # 'O' wins\n",
        "        self.done = True\n",
        "    elif game_result == 3:\n",
        "        reward = +0.1  # Draw\n",
        "        self.done = True\n",
        "    else:\n",
        "        reward = 0  # Game ongoing\n",
        "\n",
        "    # Switch to the other player\n",
        "    self.current_player = 2 if self.current_player == 1 else 1\n",
        "\n",
        "    # Return the updated state, reward, done flag, and additional info\n",
        "    return self.game_grid.copy(), reward, self.done, {}\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    # Reset the game grid and player\n",
        "    self.game_grid = np.full((3, 3), 0, dtype=np.int32)\n",
        "    self.current_player = 1  # 'X' goes first\n",
        "    self.done = False\n",
        "\n",
        "    # Return the initial observation\n",
        "    return self.game_grid.copy()\n",
        "\n",
        "  def render(self):\n",
        "    print('\\nGrid state:\\n***********************************************')\n",
        "\n",
        "    # Create a 3x3 array for rendering the grid with the appropriate symbols\n",
        "    grid_drawing = np.full((3, 3), '-', dtype=str)\n",
        "\n",
        "    # Loop through each cell in the game grid\n",
        "    for row in range(3):\n",
        "      for col in range(3):\n",
        "        # Get the value in the current cell of the game grid\n",
        "        square = self.game_grid[row, col]\n",
        "        # Convert the numerical value to the corresponding mark ('-', 'X', 'O')\n",
        "        grid_drawing[row, col] = mark_labels[square]\n",
        "\n",
        "    # Print the rendered game grid\n",
        "    for row in grid_drawing:\n",
        "        print(' '.join(row))\n",
        "    print('***********************************************\\n')\n",
        "\n",
        "  def check_game_result(self):\n",
        "    # Check rows, columns, and diagonals for a win condition\n",
        "    for i in range(3):\n",
        "        # Check rows\n",
        "        if self.game_grid[i, 0] == self.game_grid[i, 1] == self.game_grid[i, 2] and self.game_grid[i, 0] != 0:\n",
        "            return self.game_grid[i, 0]\n",
        "        # Check columns\n",
        "        if self.game_grid[0, i] == self.game_grid[1, i] == self.game_grid[2, i] and self.game_grid[0, i] != 0:\n",
        "            return self.game_grid[0, i]\n",
        "\n",
        "    # Check diagonals\n",
        "    if self.game_grid[0, 0] == self.game_grid[1, 1] == self.game_grid[2, 2] and self.game_grid[0, 0] != 0:\n",
        "        return self.game_grid[0, 0]\n",
        "    if self.game_grid[0, 2] == self.game_grid[1, 1] == self.game_grid[2, 0] and self.game_grid[0, 2] != 0:\n",
        "        return self.game_grid[0, 2]\n",
        "\n",
        "    # Check for draw (grid is full)\n",
        "    if not np.any(self.game_grid == 0):\n",
        "        return 3  # Draw\n",
        "\n",
        "    # Game ongoing\n",
        "    return 0\n"
      ],
      "metadata": {
        "id": "4ZC_FUS1oKkh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TicTacToeEnv()"
      ],
      "metadata": {
        "id": "lkyAyglXGUAd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action = env.action_space.sample()\n",
        "print(f\"marked square idx: {action}\")\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB9a7xMXGhpU",
        "outputId": "3f25a986-52c4-4aa4-9bcb-8c69160b4980"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "marked square idx: 2\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 10\n",
        "\n",
        "for episode in range(1, episodes+1):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "\n",
        "  while not done:\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "\n",
        "    # Update the current state for the next iteration\n",
        "    state = next_state\n",
        "\n",
        "  env.render()\n",
        "  print(f'Episode: {episode} Score: {score}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDQowbmxIw3F",
        "outputId": "3fc820e2-509b-4d9f-d7c4-1463397d06dc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - O\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- X O\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "- X O\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- X O\n",
            "- X O\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- X O\n",
            "- X O\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "Episode: 1 Score: 1\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "X - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - -\n",
            "X - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - -\n",
            "X - -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - -\n",
            "X O -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - -\n",
            "X O -\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - -\n",
            "X O -\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O O -\n",
            "X O -\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O O -\n",
            "X O X\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O O -\n",
            "X O X\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O O -\n",
            "X O X\n",
            "X O X\n",
            "***********************************************\n",
            "\n",
            "Episode: 2 Score: -199\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - X\n",
            "- - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "- - X\n",
            "- - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "O - X\n",
            "- - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "O - X\n",
            "- - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "O - X\n",
            "- - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "O - X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O - X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O - X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O X X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O X X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O X X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O X X\n",
            "X O O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O X X\n",
            "X O O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - O\n",
            "O X X\n",
            "X O O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O X X\n",
            "X O O\n",
            "***********************************************\n",
            "\n",
            "Episode: 3 Score: -799.9\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "- - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X - X\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X - X\n",
            "- - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - O\n",
            "X X X\n",
            "- - O\n",
            "***********************************************\n",
            "\n",
            "Episode: 4 Score: -499\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - X\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - X\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - X\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - X\n",
            "- X -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - X\n",
            "- X -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - X\n",
            "- X O\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O - X\n",
            "- X O\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "Episode: 5 Score: -199\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- - -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "X - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "X - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "X - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "X - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "X - X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "X - X\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "X X X\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "Episode: 6 Score: -699\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "- - O\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "- - O\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "- - O\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X - -\n",
            "- - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "- - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "- - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "- - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "- - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "- - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "- - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "O - O\n",
            "X O -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "O - O\n",
            "X O X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "O - O\n",
            "X O X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "O - O\n",
            "X O X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "O O O\n",
            "X O X\n",
            "***********************************************\n",
            "\n",
            "Episode: 7 Score: -799\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- - -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- - -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- - -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- X -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- X -\n",
            "- - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- X -\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- X -\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "O O O\n",
            "- X -\n",
            "X - X\n",
            "***********************************************\n",
            "\n",
            "Episode: 8 Score: -299\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- X -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- X -\n",
            "- O -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- X -\n",
            "- O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- X -\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- X -\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X -\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O -\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X - O\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "X X O\n",
            "O O X\n",
            "X X O\n",
            "***********************************************\n",
            "\n",
            "Episode: 9 Score: -1499.9\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- - -\n",
            "- - -\n",
            "- X -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- - -\n",
            "- X -\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O -\n",
            "- - -\n",
            "- X X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- - -\n",
            "- X X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- - X\n",
            "- X X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- O X\n",
            "- X X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- O X\n",
            "- X X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- O X\n",
            "- X X\n",
            "***********************************************\n",
            "\n",
            "\n",
            "Grid state:\n",
            "***********************************************\n",
            "- O O\n",
            "- O X\n",
            "X X X\n",
            "***********************************************\n",
            "\n",
            "Episode: 10 Score: -199\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Deep Learning Model"
      ],
      "metadata": {
        "id": "BE6yvjTFpPl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "AnVe5TeBqFBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten"
      ],
      "metadata": {
        "id": "ESVfFAcDpSKO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = env.observation_space.shape\n",
        "actions = env.action_space.n\n",
        "\n",
        "print(states)\n",
        "print(actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvOjY7UEp0io",
        "outputId": "a3422ebd-cc83-4cad-e951-d24c22c458e0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 3)\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create DL Model"
      ],
      "metadata": {
        "id": "braI18ZCqIAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import __version__\n",
        "import tensorflow as tf\n",
        "tf.keras.__version__ = __version__"
      ],
      "metadata": {
        "id": "cjCX6oARTT_P"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXuIbZRfRhGi",
        "outputId": "acf8ddf4-1874-45cb-bb82-75a54d6c1b3d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from tensorflow.keras.optimizers.legacy import Adam"
      ],
      "metadata": {
        "id": "BNf-ukOFhiDX"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(actions):\n",
        "    model = Sequential()\n",
        "    # Flatten the 3x3 grid to a 1D array\n",
        "    model.add(Flatten(input_shape=(1, 3, 3)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "QR7EbeTNkQt2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(actions)"
      ],
      "metadata": {
        "id": "BPH9OJQsq3ig"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUwKCqulq98v",
        "outputId": "11112c1d-4db3-4446-8a9c-31c28de2e4ad"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 9)                 0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 24)                240       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 9)                 225       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1065 (4.16 KB)\n",
            "Trainable params: 1065 (4.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Agent With Keras-RL"
      ],
      "metadata": {
        "id": "jgZ6bpiSNCKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "  #policy = BoltzmannQPolicy()\n",
        "  policy = EpsGreedyQPolicy(eps=0.6)\n",
        "  memory = SequentialMemory(limit=100000, window_length=1)\n",
        "  dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=5000, target_model_update=1e-2)\n",
        "  return dqn"
      ],
      "metadata": {
        "id": "HOwVQOGqTvdq"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=1e-2)"
      ],
      "metadata": {
        "id": "zhclBoMdb_s2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(optimizer=optimizer, metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpK7IcMTVAOq",
        "outputId": "28867e0f-eab4-4150-efd2-1d9508846489"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 61s 6ms/step - reward: -56.2068\n",
            "586 episodes - episode_reward: -959.160 [-9099.900, 1.000] - loss: 1467.696 - mae: 162.128 - mean_q: -125.792\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 92s 9ms/step - reward: -34.1763\n",
            "858 episodes - episode_reward: -398.325 [-6199.000, 1.000] - loss: 529.134 - mae: 76.447 - mean_q: -28.412\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 94s 9ms/step - reward: -31.7092\n",
            "902 episodes - episode_reward: -351.543 [-3199.000, 1.000] - loss: 359.114 - mae: 54.222 - mean_q: -3.808\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 96s 10ms/step - reward: -31.7287\n",
            "899 episodes - episode_reward: -352.822 [-2999.000, 1.000] - loss: 318.457 - mae: 51.610 - mean_q: 2.717\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 98s 10ms/step - reward: -32.7229\n",
            "881 episodes - episode_reward: -371.542 [-2699.000, 1.000] - loss: 343.046 - mae: 51.666 - mean_q: 3.951\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 102s 10ms/step - reward: -30.7876\n",
            "911 episodes - episode_reward: -337.515 [-2699.000, 1.000] - loss: 331.226 - mae: 49.939 - mean_q: 5.632\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 104s 10ms/step - reward: -31.2180\n",
            "924 episodes - episode_reward: -338.290 [-2999.000, 1.000] - loss: 309.414 - mae: 48.548 - mean_q: 8.351\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 104s 10ms/step - reward: -33.8775\n",
            "893 episodes - episode_reward: -379.368 [-2399.000, 1.000] - loss: 303.313 - mae: 49.331 - mean_q: 10.570\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: -30.5282\n",
            "914 episodes - episode_reward: -333.897 [-2399.000, 1.000] - loss: 298.624 - mae: 49.362 - mean_q: 9.219\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -31.6393\n",
            "912 episodes - episode_reward: -347.032 [-2699.000, 1.000] - loss: 298.362 - mae: 48.657 - mean_q: 10.446\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -32.4413\n",
            "894 episodes - episode_reward: -362.878 [-3099.000, 1.000] - loss: 285.286 - mae: 45.987 - mean_q: 11.851\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -32.5905\n",
            "889 episodes - episode_reward: -366.597 [-3899.000, 1.000] - loss: 289.815 - mae: 46.936 - mean_q: 13.152\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -29.4672\n",
            "926 episodes - episode_reward: -318.220 [-2299.000, 1.000] - loss: 202.800 - mae: 46.208 - mean_q: 14.180\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -29.0698\n",
            "933 episodes - episode_reward: -311.574 [-2599.000, 1.000] - loss: 129.890 - mae: 46.499 - mean_q: 18.262\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.9772\n",
            "927 episodes - episode_reward: -312.052 [-2699.000, 1.000] - loss: 131.613 - mae: 47.201 - mean_q: 22.164\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -29.3723\n",
            "908 episodes - episode_reward: -323.704 [-2299.900, 1.000] - loss: 124.712 - mae: 47.724 - mean_q: 21.132\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -30.3504\n",
            "901 episodes - episode_reward: -336.853 [-3399.000, 1.000] - loss: 130.224 - mae: 48.317 - mean_q: 21.683\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.5344\n",
            "943 episodes - episode_reward: -302.698 [-2299.900, 1.000] - loss: 113.993 - mae: 48.387 - mean_q: 23.172\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -27.4927\n",
            "952 episodes - episode_reward: -288.894 [-2099.000, 1.000] - loss: 128.185 - mae: 48.774 - mean_q: 21.923\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 116s 12ms/step - reward: -27.7150\n",
            "944 episodes - episode_reward: -293.486 [-1699.000, 1.000] - loss: 117.563 - mae: 48.260 - mean_q: 22.119\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -27.7175\n",
            "949 episodes - episode_reward: -290.174 [-3599.000, 1.000] - loss: 124.045 - mae: 48.348 - mean_q: 23.875\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.1248\n",
            "940 episodes - episode_reward: -301.009 [-3599.000, 1.000] - loss: 123.989 - mae: 48.023 - mean_q: 22.062\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -29.8667\n",
            "923 episodes - episode_reward: -323.041 [-5299.900, 1.000] - loss: 117.441 - mae: 47.750 - mean_q: 21.914\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.3178\n",
            "946 episodes - episode_reward: -289.618 [-2699.000, 1.000] - loss: 109.145 - mae: 48.254 - mean_q: 22.333\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.4169\n",
            "945 episodes - episode_reward: -300.391 [-2499.000, 1.000] - loss: 117.266 - mae: 48.834 - mean_q: 23.997\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -28.4182\n",
            "923 episodes - episode_reward: -308.107 [-2199.000, 1.000] - loss: 115.643 - mae: 48.217 - mean_q: 23.008\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -29.2990\n",
            "918 episodes - episode_reward: -319.052 [-4199.000, 1.000] - loss: 115.804 - mae: 48.008 - mean_q: 22.628\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -29.9771\n",
            "932 episodes - episode_reward: -321.643 [-5399.900, 1.000] - loss: 125.914 - mae: 50.489 - mean_q: 25.975\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -29.8790\n",
            "912 episodes - episode_reward: -327.840 [-2999.000, 1.000] - loss: 122.438 - mae: 49.846 - mean_q: 27.139\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -27.7343\n",
            "947 episodes - episode_reward: -292.865 [-3999.000, 1.000] - loss: 114.742 - mae: 50.262 - mean_q: 24.319\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.3401\n",
            "912 episodes - episode_reward: -321.712 [-5499.000, 1.000] - loss: 121.938 - mae: 49.947 - mean_q: 22.908\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -29.2387\n",
            "934 episodes - episode_reward: -313.049 [-2599.000, 1.000] - loss: 119.571 - mae: 49.910 - mean_q: 24.528\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -29.8072\n",
            "910 episodes - episode_reward: -327.552 [-3199.000, 1.000] - loss: 127.870 - mae: 50.605 - mean_q: 24.115\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.4087\n",
            "942 episodes - episode_reward: -290.963 [-1599.000, 1.000] - loss: 120.895 - mae: 50.070 - mean_q: 24.088\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 115s 12ms/step - reward: -29.1921\n",
            "909 episodes - episode_reward: -320.926 [-2799.000, 1.000] - loss: 125.135 - mae: 49.648 - mean_q: 22.269\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.4153\n",
            "926 episodes - episode_reward: -317.444 [-4499.000, 1.000] - loss: 136.873 - mae: 50.114 - mean_q: 21.944\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.7229\n",
            "954 episodes - episode_reward: -291.016 [-2599.900, 1.000] - loss: 126.071 - mae: 50.077 - mean_q: 24.424\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.9664\n",
            "921 episodes - episode_reward: -325.043 [-7899.000, 1.000] - loss: 119.061 - mae: 49.592 - mean_q: 24.784\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.6478\n",
            "936 episodes - episode_reward: -306.280 [-2399.000, 1.000] - loss: 125.000 - mae: 50.613 - mean_q: 26.013\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -28.4185\n",
            "930 episodes - episode_reward: -305.575 [-1599.000, 1.000] - loss: 122.317 - mae: 49.996 - mean_q: 24.640\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -27.2164\n",
            "950 episodes - episode_reward: -286.594 [-2099.000, 1.000] - loss: 119.011 - mae: 49.886 - mean_q: 25.112\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.5728\n",
            "949 episodes - episode_reward: -290.336 [-2299.000, 1.000] - loss: 130.990 - mae: 49.641 - mean_q: 24.274\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.0556\n",
            "940 episodes - episode_reward: -309.102 [-2199.000, 1.000] - loss: 117.582 - mae: 49.245 - mean_q: 25.384\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -27.4439\n",
            "965 episodes - episode_reward: -284.600 [-2399.900, 1.000] - loss: 121.442 - mae: 49.547 - mean_q: 24.115\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.5949\n",
            "954 episodes - episode_reward: -289.149 [-2599.000, 1.000] - loss: 112.740 - mae: 49.383 - mean_q: 25.899\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 115s 12ms/step - reward: -29.3142\n",
            "935 episodes - episode_reward: -313.627 [-3299.900, 1.000] - loss: 125.754 - mae: 49.381 - mean_q: 25.018\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 115s 12ms/step - reward: -29.5279\n",
            "920 episodes - episode_reward: -320.955 [-9299.000, 1.000] - loss: 132.266 - mae: 49.510 - mean_q: 23.473\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.3663\n",
            "965 episodes - episode_reward: -283.485 [-1499.000, 1.000] - loss: 111.770 - mae: 48.747 - mean_q: 24.422\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -27.1730\n",
            "965 episodes - episode_reward: -281.482 [-1699.000, 1.000] - loss: 116.158 - mae: 48.857 - mean_q: 24.289\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.2053\n",
            "941 episodes - episode_reward: -299.844 [-1999.900, 1.000] - loss: 118.283 - mae: 49.712 - mean_q: 26.624\n",
            "\n",
            "Interval 51 (500000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.6871\n",
            "953 episodes - episode_reward: -290.316 [-1899.900, 1.000] - loss: 123.007 - mae: 49.490 - mean_q: 26.005\n",
            "\n",
            "Interval 52 (510000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -28.9500\n",
            "935 episodes - episode_reward: -309.733 [-3599.000, 1.000] - loss: 118.985 - mae: 49.513 - mean_q: 26.196\n",
            "\n",
            "Interval 53 (520000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.5876\n",
            "928 episodes - episode_reward: -307.625 [-2199.000, 1.000] - loss: 117.170 - mae: 49.933 - mean_q: 25.421\n",
            "\n",
            "Interval 54 (530000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.2007\n",
            "977 episodes - episode_reward: -278.922 [-3099.900, 1.000] - loss: 110.817 - mae: 49.454 - mean_q: 26.118\n",
            "\n",
            "Interval 55 (540000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -27.2136\n",
            "960 episodes - episode_reward: -283.371 [-1799.000, 1.000] - loss: 115.030 - mae: 49.775 - mean_q: 24.549\n",
            "\n",
            "Interval 56 (550000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.2395\n",
            "921 episodes - episode_reward: -317.584 [-3699.000, 1.000] - loss: 129.618 - mae: 49.907 - mean_q: 25.483\n",
            "\n",
            "Interval 57 (560000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -26.1802\n",
            "980 episodes - episode_reward: -267.247 [-2499.900, 1.000] - loss: 126.827 - mae: 48.937 - mean_q: 24.982\n",
            "\n",
            "Interval 58 (570000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.0835\n",
            "957 episodes - episode_reward: -293.244 [-3199.000, 1.000] - loss: 117.839 - mae: 49.070 - mean_q: 26.119\n",
            "\n",
            "Interval 59 (580000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.1266\n",
            "951 episodes - episode_reward: -295.968 [-3899.900, 1.000] - loss: 117.841 - mae: 48.788 - mean_q: 23.808\n",
            "\n",
            "Interval 60 (590000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -27.2373\n",
            "950 episodes - episode_reward: -286.709 [-1599.000, 1.000] - loss: 115.143 - mae: 49.567 - mean_q: 26.428\n",
            "\n",
            "Interval 61 (600000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -26.7953\n",
            "976 episodes - episode_reward: -274.337 [-1999.000, 1.000] - loss: 121.108 - mae: 48.646 - mean_q: 22.924\n",
            "\n",
            "Interval 62 (610000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.4812\n",
            "926 episodes - episode_reward: -318.479 [-2999.000, 1.000] - loss: 114.571 - mae: 49.449 - mean_q: 25.332\n",
            "\n",
            "Interval 63 (620000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -30.0904\n",
            "899 episodes - episode_reward: -334.820 [-2999.000, 1.000] - loss: 125.924 - mae: 48.973 - mean_q: 26.259\n",
            "\n",
            "Interval 64 (630000 steps performed)\n",
            "10000/10000 [==============================] - 116s 12ms/step - reward: -26.9254\n",
            "948 episodes - episode_reward: -283.495 [-1699.000, 1.000] - loss: 119.083 - mae: 48.879 - mean_q: 25.234\n",
            "\n",
            "Interval 65 (640000 steps performed)\n",
            "10000/10000 [==============================] - 116s 12ms/step - reward: -28.1164\n",
            "943 episodes - episode_reward: -298.689 [-2199.900, 1.000] - loss: 116.568 - mae: 49.242 - mean_q: 26.468\n",
            "\n",
            "Interval 66 (650000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.0241\n",
            "945 episodes - episode_reward: -295.917 [-2699.900, 1.000] - loss: 117.161 - mae: 46.482 - mean_q: 19.260\n",
            "\n",
            "Interval 67 (660000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.9055\n",
            "936 episodes - episode_reward: -309.460 [-3299.000, 1.000] - loss: 115.454 - mae: 49.174 - mean_q: 23.177\n",
            "\n",
            "Interval 68 (670000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -30.0874\n",
            "915 episodes - episode_reward: -328.824 [-4899.900, 1.000] - loss: 114.403 - mae: 48.866 - mean_q: 25.346\n",
            "\n",
            "Interval 69 (680000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.4165\n",
            "939 episodes - episode_reward: -302.519 [-3299.000, 1.000] - loss: 124.450 - mae: 48.896 - mean_q: 22.037\n",
            "\n",
            "Interval 70 (690000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -29.7267\n",
            "925 episodes - episode_reward: -321.370 [-3299.900, 1.000] - loss: 121.042 - mae: 50.242 - mean_q: 26.944\n",
            "\n",
            "Interval 71 (700000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.9077\n",
            "924 episodes - episode_reward: -312.962 [-4999.000, 1.000] - loss: 122.934 - mae: 50.483 - mean_q: 24.899\n",
            "\n",
            "Interval 72 (710000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -30.1268\n",
            "927 episodes - episode_reward: -324.992 [-3499.000, 1.000] - loss: 134.380 - mae: 50.959 - mean_q: 25.936\n",
            "\n",
            "Interval 73 (720000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -27.9557\n",
            "970 episodes - episode_reward: -287.997 [-1899.000, 1.000] - loss: 114.141 - mae: 49.938 - mean_q: 23.746\n",
            "\n",
            "Interval 74 (730000 steps performed)\n",
            "10000/10000 [==============================] - 115s 12ms/step - reward: -26.9253\n",
            "972 episodes - episode_reward: -277.112 [-2899.000, 1.000] - loss: 111.504 - mae: 49.645 - mean_q: 23.125\n",
            "\n",
            "Interval 75 (740000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -27.9371\n",
            "954 episodes - episode_reward: -292.947 [-2699.900, 1.000] - loss: 125.517 - mae: 49.123 - mean_q: 20.042\n",
            "\n",
            "Interval 76 (750000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -30.3400\n",
            "914 episodes - episode_reward: -331.948 [-4099.000, 1.000] - loss: 124.908 - mae: 49.053 - mean_q: 22.074\n",
            "\n",
            "Interval 77 (760000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.1583\n",
            "929 episodes - episode_reward: -302.672 [-1899.900, 1.000] - loss: 121.660 - mae: 50.394 - mean_q: 26.470\n",
            "\n",
            "Interval 78 (770000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -28.7174\n",
            "922 episodes - episode_reward: -311.686 [-2799.000, 1.000] - loss: 123.142 - mae: 49.663 - mean_q: 24.799\n",
            "\n",
            "Interval 79 (780000 steps performed)\n",
            "10000/10000 [==============================] - 115s 12ms/step - reward: -27.5047\n",
            "961 episodes - episode_reward: -286.417 [-2599.000, 1.000] - loss: 119.216 - mae: 49.865 - mean_q: 24.775\n",
            "\n",
            "Interval 80 (790000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -29.0716\n",
            "927 episodes - episode_reward: -309.834 [-2199.000, 1.000] - loss: 119.663 - mae: 49.941 - mean_q: 24.660\n",
            "\n",
            "Interval 81 (800000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -30.1888\n",
            "914 episodes - episode_reward: -334.122 [-3999.000, 1.000] - loss: 118.678 - mae: 49.951 - mean_q: 24.317\n",
            "\n",
            "Interval 82 (810000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -29.9811\n",
            "908 episodes - episode_reward: -329.968 [-5999.900, 1.000] - loss: 121.015 - mae: 49.947 - mean_q: 26.084\n",
            "\n",
            "Interval 83 (820000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -27.4643\n",
            "960 episodes - episode_reward: -286.294 [-2399.900, 1.000] - loss: 127.334 - mae: 51.336 - mean_q: 28.036\n",
            "\n",
            "Interval 84 (830000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.8965\n",
            "934 episodes - episode_reward: -309.384 [-4099.900, 1.000] - loss: 123.157 - mae: 50.141 - mean_q: 24.705\n",
            "\n",
            "Interval 85 (840000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -27.6248\n",
            "948 episodes - episode_reward: -291.401 [-2099.000, 1.000] - loss: 116.097 - mae: 50.617 - mean_q: 26.322\n",
            "\n",
            "Interval 86 (850000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.0953\n",
            "952 episodes - episode_reward: -295.119 [-1699.900, 1.000] - loss: 118.415 - mae: 50.701 - mean_q: 25.373\n",
            "\n",
            "Interval 87 (860000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.2665\n",
            "925 episodes - episode_reward: -316.395 [-2799.000, 1.000] - loss: 120.160 - mae: 50.241 - mean_q: 26.424\n",
            "\n",
            "Interval 88 (870000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -28.8378\n",
            "938 episodes - episode_reward: -307.439 [-5199.000, 1.000] - loss: 124.524 - mae: 50.149 - mean_q: 26.984\n",
            "\n",
            "Interval 89 (880000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.5732\n",
            "941 episodes - episode_reward: -303.435 [-3999.000, 1.000] - loss: 123.877 - mae: 50.845 - mean_q: 26.603\n",
            "\n",
            "Interval 90 (890000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.1186\n",
            "939 episodes - episode_reward: -299.453 [-1999.000, 1.000] - loss: 116.151 - mae: 50.271 - mean_q: 24.808\n",
            "\n",
            "Interval 91 (900000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -26.9476\n",
            "962 episodes - episode_reward: -280.328 [-2399.000, 1.000] - loss: 118.040 - mae: 50.349 - mean_q: 26.440\n",
            "\n",
            "Interval 92 (910000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -27.5463\n",
            "951 episodes - episode_reward: -289.236 [-3099.000, 1.000] - loss: 131.571 - mae: 49.780 - mean_q: 25.516\n",
            "\n",
            "Interval 93 (920000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -27.6483\n",
            "955 episodes - episode_reward: -289.930 [-3299.000, 1.000] - loss: 122.311 - mae: 49.726 - mean_q: 23.663\n",
            "\n",
            "Interval 94 (930000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -28.1259\n",
            "952 episodes - episode_reward: -295.440 [-3099.000, 1.000] - loss: 126.363 - mae: 50.464 - mean_q: 24.946\n",
            "\n",
            "Interval 95 (940000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -28.9175\n",
            "928 episodes - episode_reward: -311.612 [-4199.000, 1.000] - loss: 120.865 - mae: 49.764 - mean_q: 24.423\n",
            "\n",
            "Interval 96 (950000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -27.4028\n",
            "959 episodes - episode_reward: -285.744 [-1699.000, 1.000] - loss: 113.985 - mae: 50.403 - mean_q: 25.687\n",
            "\n",
            "Interval 97 (960000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -29.0594\n",
            "913 episodes - episode_reward: -318.175 [-3099.000, 1.000] - loss: 126.303 - mae: 50.296 - mean_q: 25.061\n",
            "\n",
            "Interval 98 (970000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -30.5429\n",
            "902 episodes - episode_reward: -338.502 [-7099.900, 1.000] - loss: 125.155 - mae: 48.688 - mean_q: 24.248\n",
            "\n",
            "Interval 99 (980000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -26.4034\n",
            "968 episodes - episode_reward: -272.969 [-1599.000, 1.000] - loss: 117.922 - mae: 50.624 - mean_q: 24.314\n",
            "\n",
            "Interval 100 (990000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -28.7856\n",
            "done, took 11177.162 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bf09961ff70>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StFUJYfIjTMr",
        "outputId": "324e2683-e7e5-4330-a5a4-d21231e6d902"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: 1.000, steps: 7\n",
            "Episode 2: reward: 1.000, steps: 7\n",
            "Episode 3: reward: 1.000, steps: 7\n",
            "Episode 4: reward: 1.000, steps: 7\n",
            "Episode 5: reward: 1.000, steps: 7\n",
            "Episode 6: reward: 1.000, steps: 7\n",
            "Episode 7: reward: 1.000, steps: 7\n",
            "Episode 8: reward: 1.000, steps: 7\n",
            "Episode 9: reward: 1.000, steps: 7\n",
            "Episode 10: reward: 1.000, steps: 7\n",
            "Episode 11: reward: 1.000, steps: 7\n",
            "Episode 12: reward: 1.000, steps: 7\n",
            "Episode 13: reward: 1.000, steps: 7\n",
            "Episode 14: reward: 1.000, steps: 7\n",
            "Episode 15: reward: 1.000, steps: 7\n",
            "Episode 16: reward: 1.000, steps: 7\n",
            "Episode 17: reward: 1.000, steps: 7\n",
            "Episode 18: reward: 1.000, steps: 7\n",
            "Episode 19: reward: 1.000, steps: 7\n",
            "Episode 20: reward: 1.000, steps: 7\n",
            "Episode 21: reward: 1.000, steps: 7\n",
            "Episode 22: reward: 1.000, steps: 7\n",
            "Episode 23: reward: 1.000, steps: 7\n",
            "Episode 24: reward: 1.000, steps: 7\n",
            "Episode 25: reward: 1.000, steps: 7\n",
            "Episode 26: reward: 1.000, steps: 7\n",
            "Episode 27: reward: 1.000, steps: 7\n",
            "Episode 28: reward: 1.000, steps: 7\n",
            "Episode 29: reward: 1.000, steps: 7\n",
            "Episode 30: reward: 1.000, steps: 7\n",
            "Episode 31: reward: 1.000, steps: 7\n",
            "Episode 32: reward: 1.000, steps: 7\n",
            "Episode 33: reward: 1.000, steps: 7\n",
            "Episode 34: reward: 1.000, steps: 7\n",
            "Episode 35: reward: 1.000, steps: 7\n",
            "Episode 36: reward: 1.000, steps: 7\n",
            "Episode 37: reward: 1.000, steps: 7\n",
            "Episode 38: reward: 1.000, steps: 7\n",
            "Episode 39: reward: 1.000, steps: 7\n",
            "Episode 40: reward: 1.000, steps: 7\n",
            "Episode 41: reward: 1.000, steps: 7\n",
            "Episode 42: reward: 1.000, steps: 7\n",
            "Episode 43: reward: 1.000, steps: 7\n",
            "Episode 44: reward: 1.000, steps: 7\n",
            "Episode 45: reward: 1.000, steps: 7\n",
            "Episode 46: reward: 1.000, steps: 7\n",
            "Episode 47: reward: 1.000, steps: 7\n",
            "Episode 48: reward: 1.000, steps: 7\n",
            "Episode 49: reward: 1.000, steps: 7\n",
            "Episode 50: reward: 1.000, steps: 7\n",
            "Episode 51: reward: 1.000, steps: 7\n",
            "Episode 52: reward: 1.000, steps: 7\n",
            "Episode 53: reward: 1.000, steps: 7\n",
            "Episode 54: reward: 1.000, steps: 7\n",
            "Episode 55: reward: 1.000, steps: 7\n",
            "Episode 56: reward: 1.000, steps: 7\n",
            "Episode 57: reward: 1.000, steps: 7\n",
            "Episode 58: reward: 1.000, steps: 7\n",
            "Episode 59: reward: 1.000, steps: 7\n",
            "Episode 60: reward: 1.000, steps: 7\n",
            "Episode 61: reward: 1.000, steps: 7\n",
            "Episode 62: reward: 1.000, steps: 7\n",
            "Episode 63: reward: 1.000, steps: 7\n",
            "Episode 64: reward: 1.000, steps: 7\n",
            "Episode 65: reward: 1.000, steps: 7\n",
            "Episode 66: reward: 1.000, steps: 7\n",
            "Episode 67: reward: 1.000, steps: 7\n",
            "Episode 68: reward: 1.000, steps: 7\n",
            "Episode 69: reward: 1.000, steps: 7\n",
            "Episode 70: reward: 1.000, steps: 7\n",
            "Episode 71: reward: 1.000, steps: 7\n",
            "Episode 72: reward: 1.000, steps: 7\n",
            "Episode 73: reward: 1.000, steps: 7\n",
            "Episode 74: reward: 1.000, steps: 7\n",
            "Episode 75: reward: 1.000, steps: 7\n",
            "Episode 76: reward: 1.000, steps: 7\n",
            "Episode 77: reward: 1.000, steps: 7\n",
            "Episode 78: reward: 1.000, steps: 7\n",
            "Episode 79: reward: 1.000, steps: 7\n",
            "Episode 80: reward: 1.000, steps: 7\n",
            "Episode 81: reward: 1.000, steps: 7\n",
            "Episode 82: reward: 1.000, steps: 7\n",
            "Episode 83: reward: 1.000, steps: 7\n",
            "Episode 84: reward: 1.000, steps: 7\n",
            "Episode 85: reward: 1.000, steps: 7\n",
            "Episode 86: reward: 1.000, steps: 7\n",
            "Episode 87: reward: 1.000, steps: 7\n",
            "Episode 88: reward: 1.000, steps: 7\n",
            "Episode 89: reward: 1.000, steps: 7\n",
            "Episode 90: reward: 1.000, steps: 7\n",
            "Episode 91: reward: 1.000, steps: 7\n",
            "Episode 92: reward: 1.000, steps: 7\n",
            "Episode 93: reward: 1.000, steps: 7\n",
            "Episode 94: reward: 1.000, steps: 7\n",
            "Episode 95: reward: 1.000, steps: 7\n",
            "Episode 96: reward: 1.000, steps: 7\n",
            "Episode 97: reward: 1.000, steps: 7\n",
            "Episode 98: reward: 1.000, steps: 7\n",
            "Episode 99: reward: 1.000, steps: 7\n",
            "Episode 100: reward: 1.000, steps: 7\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save The Model And Agent"
      ],
      "metadata": {
        "id": "xU8Bq2Em2FPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLyS6zFc24u9",
        "outputId": "48ccd867-7517-4874-df72-ff84ae87cd8f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Machine Learning Projects/DL Practice/Tic Tac Toe RL\""
      ],
      "metadata": {
        "id": "EaAV8YP02Xyt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "N84bt9zY2tFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(f\"{path}/tic_tac_toe_model\", exist_ok=True)\n",
        "\n",
        "model.save(f\"{path}/tic_tac_toe_model/tic_tac_toe_model.h5\")"
      ],
      "metadata": {
        "id": "zm5vjMfh2C_m"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Agent"
      ],
      "metadata": {
        "id": "qxxv2ojF2yUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Get the agent's configuration dictionary\n",
        "agent_config = dqn.get_config()\n",
        "\n",
        "# Save the agent's configuration to a JSON file\n",
        "with open(f\"{path}/tic_tac_toe_agent_weights/tic_tac_toe_agent_config.json\", \"w\") as config_out:\n",
        "    json.dump(agent_config, config_out)\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(f\"{path}/tic_tac_toe_agent_weights\", exist_ok=True)\n",
        "\n",
        "# Save the agent's weights\n",
        "dqn.save_weights(f\"{path}/tic_tac_toe_agent_weights/tic_tac_toe_agent_weights.h5f\", overwrite=True)"
      ],
      "metadata": {
        "id": "CzV5L4Wz2VBW"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model And Agent"
      ],
      "metadata": {
        "id": "CnUh82lDGusO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Environment For Model And Agent Testing"
      ],
      "metadata": {
        "id": "2hWuITz4A5IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import __version__\n",
        "import tensorflow as tf\n",
        "tf.keras.__version__ = __version__"
      ],
      "metadata": {
        "id": "gDMRfaDXIRXd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RABKCnBVIg6c",
        "outputId": "ce841782-cd3a-421d-884c-0604211d4eb6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m998.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->keras-rl2) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "import json"
      ],
      "metadata": {
        "id": "OLLjEjN9HdLt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qgn0NylHWvF",
        "outputId": "66ef688c-7f68-4514-bc6d-7ac26700b361"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Machine Learning Projects/DL Practice/Tic Tac Toe RL\""
      ],
      "metadata": {
        "id": "GMozCHDxHYjJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions):\n",
        "  #policy = BoltzmannQPolicy()\n",
        "  policy = EpsGreedyQPolicy(eps=0.6)\n",
        "  memory = SequentialMemory(limit=50000, window_length=1)\n",
        "  dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
        "  return dqn"
      ],
      "metadata": {
        "id": "IS6y8RFrH04o"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_config\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "\n",
        "# Load the agent's configuration from the JSON file\n",
        "with open(f\"{path}/tic_tac_toe_agent_weights/tic_tac_toe_agent_config.json\", \"r\") as config_in:\n",
        "    config = json.load(config_in)\n",
        "\n",
        "# Load the agent's model\n",
        "model = model_from_config(config[\"model\"])\n",
        "\n",
        "# Load the agent's weights\n",
        "model.load_weights(f\"{path}/tic_tac_toe_agent_weights/tic_tac_toe_agent_weights.h5f\")\n",
        "\n",
        "# Rebuild the agent with the loaded model and its configuration\n",
        "dqn = build_agent(model, actions)"
      ],
      "metadata": {
        "id": "3T-BjGVzE9jX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = env.observation_space.shape\n",
        "actions = env.action_space.n"
      ],
      "metadata": {
        "id": "Xt_X6RsMH_m9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_config\n",
        "\n",
        "# Load the agent's configuration from the JSON file\n",
        "with open(f\"{path}/tic_tac_toe_agent_weights/tic_tac_toe_agent_config.json\", \"r\") as config_in:\n",
        "    config = json.load(config_in)\n",
        "\n",
        "# Load the agent's model\n",
        "model = model_from_config(config[\"model\"])\n",
        "\n",
        "# Load the agent's weights\n",
        "model.load_weights(f\"{path}/tic_tac_toe_agent_weights/tic_tac_toe_agent_weights.h5f\")\n",
        "\n",
        "# Rebuild the agent with the loaded model and its configuration\n",
        "dqn = build_agent(model, actions)"
      ],
      "metadata": {
        "id": "wn3zrBC4Hb5F"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define state labels for tic tac toe game cells ('-' (empty), 'X' and 'O')\n",
        "mark_labels = ['-', 'X', 'O']\n",
        "# Define game result labels (0 - game ongoing, 1 - 'X' won, 2 - 'O' won, 3 - draw)\n",
        "game_result_labels = ['ongoing', 'X-won', 'O-won', 'draw']"
      ],
      "metadata": {
        "id": "J4AGx5_RBV8-"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(actions):\n",
        "    model = Sequential()\n",
        "    # Flatten the 3x3 grid to a 1D array\n",
        "    model.add(Flatten(input_shape=(1, 3, 3)))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "W5GohbZ4Hw3a"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent VS Agent Environment"
      ],
      "metadata": {
        "id": "-G1xl4J1P4AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentVSAgentEnv(Env):\n",
        "  def __init__(self):\n",
        "    # Actions we can take - Square in grid to mark (1-9)\n",
        "    self.action_space = Discrete(9)\n",
        "    # Observation space: 3x3 grid with 3 possible values (-, X, O) encoded as integers (0, 1, 2)\n",
        "    self.observation_space = Box(low=0, high=2, shape=(3, 3), dtype=np.int32)\n",
        "    # Initialize the game grid\n",
        "    self.game_grid = np.full((3, 3), 0, dtype=np.int32)\n",
        "    # Set starting player as the index of 'X' (1)\n",
        "    self.current_player = 1 # 1 for 'X', 2 for 'O'\n",
        "    # Episode status\n",
        "    self.done = False\n",
        "\n",
        "  def step(self, action):\n",
        "    # Convert action (0-8) to row and column indices (0-2)\n",
        "    row, col = divmod(action, 3)\n",
        "\n",
        "    # Check if the chosen sqaure is empty\n",
        "    if self.game_grid[row, col] != 0:\n",
        "      # Invalid action, return a large negative reward\n",
        "      return self.game_grid, -10, False, {}\n",
        "\n",
        "    # Mark the chosen square with the current player's mark\n",
        "    self.game_grid[row, col] = self.current_player\n",
        "\n",
        "    # Check the game result\n",
        "    game_result = self.check_game_result()\n",
        "\n",
        "    # Determine the reward based on the game result\n",
        "    if game_result == 1:\n",
        "        reward = 1 if self.current_player == 1 else -2  # 'X' wins\n",
        "        self.done = True\n",
        "    elif game_result == 2:\n",
        "        reward = -2 if self.current_player == 1 else 1  # 'O' wins\n",
        "        self.done = True\n",
        "    elif game_result == 3:\n",
        "        reward = -1  # Draw\n",
        "        self.done = True\n",
        "    else:\n",
        "        reward = 0  # Game ongoing\n",
        "\n",
        "    # Switch to the other player\n",
        "    self.current_player = 2 if self.current_player == 1 else 1\n",
        "\n",
        "    # Return the updated state, reward, done flag, and additional info\n",
        "    return self.game_grid.copy(), reward, self.done, {}\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    # Reset the game grid and player\n",
        "    self.game_grid = np.full((3, 3), 0, dtype=np.int32)\n",
        "    self.current_player = 1  # 'X' goes first\n",
        "    self.done = False\n",
        "\n",
        "    # Return the initial observation\n",
        "    return self.game_grid.copy()\n",
        "\n",
        "  def render(self):\n",
        "    print('\\nGrid state:\\n******')\n",
        "\n",
        "    # Create a 3x3 array for rendering the grid with the appropriate symbols\n",
        "    grid_drawing = np.full((3, 3), '-', dtype=str)\n",
        "\n",
        "    # Loop through each cell in the game grid\n",
        "    for row in range(3):\n",
        "      for col in range(3):\n",
        "        # Get the value in the current cell of the game grid\n",
        "        square = self.game_grid[row, col]\n",
        "        # Convert the numerical value to the corresponding mark ('-', 'X', 'O')\n",
        "        grid_drawing[row, col] = mark_labels[square]\n",
        "\n",
        "    # Print the rendered game grid\n",
        "    for row in grid_drawing:\n",
        "        print(' '.join(row))\n",
        "    print('******\\n')\n",
        "\n",
        "  def check_game_result(self):\n",
        "    # Check rows, columns, and diagonals for a win condition\n",
        "    for i in range(3):\n",
        "        # Check rows\n",
        "        if self.game_grid[i, 0] == self.game_grid[i, 1] == self.game_grid[i, 2] and self.game_grid[i, 0] != 0:\n",
        "            return self.game_grid[i, 0]\n",
        "        # Check columns\n",
        "        if self.game_grid[0, i] == self.game_grid[1, i] == self.game_grid[2, i] and self.game_grid[0, i] != 0:\n",
        "            return self.game_grid[0, i]\n",
        "\n",
        "    # Check diagonals\n",
        "    if self.game_grid[0, 0] == self.game_grid[1, 1] == self.game_grid[2, 2] and self.game_grid[0, 0] != 0:\n",
        "        return self.game_grid[0, 0]\n",
        "    if self.game_grid[0, 2] == self.game_grid[1, 1] == self.game_grid[2, 0] and self.game_grid[0, 2] != 0:\n",
        "        return self.game_grid[0, 2]\n",
        "\n",
        "    # Check for draw (grid is full)\n",
        "    if not np.any(self.game_grid == 0):\n",
        "        return 3  # Draw\n",
        "\n",
        "    # Game ongoing\n",
        "    return 0\n"
      ],
      "metadata": {
        "id": "GvlTFFDTBZOy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = AgentVSAgentEnv()"
      ],
      "metadata": {
        "id": "2irY7n3nBbkp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "done = False\n",
        "score = 0\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    action = dqn.forward(state)  # Use forward method instead of act\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    score += reward\n",
        "\n",
        "    # Update the current state for the next iteration\n",
        "    state = next_state\n",
        "\n",
        "env.render()\n",
        "print(f'Score: {score}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKro8lWqBvfa",
        "outputId": "587833cd-e639-4dbb-b8c4-700c5d35a08b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grid state:\n",
            "******\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "******\n",
            "\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- X -\n",
            "- - -\n",
            "- - -\n",
            "******\n",
            "\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- X -\n",
            "- - -\n",
            "- - O\n",
            "******\n",
            "\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- X -\n",
            "X - -\n",
            "- - O\n",
            "******\n",
            "\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- X O\n",
            "X - -\n",
            "- - O\n",
            "******\n",
            "\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- X O\n",
            "X X -\n",
            "- - O\n",
            "******\n",
            "\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "O X O\n",
            "X X -\n",
            "- - O\n",
            "******\n",
            "\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "O X O\n",
            "X X -\n",
            "- X O\n",
            "******\n",
            "\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent VS Human Environment"
      ],
      "metadata": {
        "id": "voWfndAtP_pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentVSHumanEnv(Env):\n",
        "    def __init__(self):\n",
        "        # Actions we can take - Square in grid to mark (1-9)\n",
        "        self.action_space = Discrete(9)\n",
        "        # Observation space: 3x3 grid with 3 possible values (-, X, O) encoded as integers (0, 1, 2)\n",
        "        self.observation_space = Box(low=0, high=2, shape=(3, 3), dtype=np.int32)\n",
        "        # Initialize the game grid\n",
        "        self.game_grid = np.full((3, 3), 0, dtype=np.int32)\n",
        "        # Set starting player as the index of 'X' (1)\n",
        "        self.current_player = 1 # 1 for 'X', 2 for 'O'\n",
        "        # Episode status\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        # Convert action (0-8) to row and column indices (0-2)\n",
        "        row, col = divmod(action, 3)\n",
        "\n",
        "        # Check if the chosen square is empty\n",
        "        if self.game_grid[row, col] != 0:\n",
        "            # Invalid action, return a large negative reward\n",
        "            return self.game_grid, -10, False, {}\n",
        "\n",
        "        # Mark the chosen square with the current player's mark\n",
        "        self.game_grid[row, col] = self.current_player\n",
        "\n",
        "        # Check the game result\n",
        "        game_result = self.check_game_result()\n",
        "\n",
        "        # Determine the reward based on the game result\n",
        "        if game_result == 1:\n",
        "            reward = 1 if self.current_player == 1 else -2  # 'X' wins\n",
        "            self.done = True\n",
        "        elif game_result == 2:\n",
        "            reward = -2 if self.current_player == 1 else 1  # 'O' wins\n",
        "            self.done = True\n",
        "        elif game_result == 3:\n",
        "            reward = -1  # Draw\n",
        "            self.done = True\n",
        "        else:\n",
        "            reward = 0  # Game ongoing\n",
        "\n",
        "        # Switch to the other player\n",
        "        self.current_player = 2 if self.current_player == 1 else 1\n",
        "\n",
        "        # Return the updated state, reward, done flag, and additional info\n",
        "        return self.game_grid.copy(), reward, self.done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the game grid and player\n",
        "        self.game_grid = np.full((3, 3), 0, dtype=np.int32)\n",
        "        self.current_player = 1  # 'X' goes first\n",
        "        self.done = False\n",
        "\n",
        "        # Return the initial observation\n",
        "        return self.game_grid.copy()\n",
        "\n",
        "    def render(self):\n",
        "        print('\\nGrid state:\\n******')\n",
        "\n",
        "        # Create a 3x3 array for rendering the grid with the appropriate symbols\n",
        "        grid_drawing = np.full((3, 3), '-', dtype=str)\n",
        "\n",
        "        # Loop through each cell in the game grid\n",
        "        for row in range(3):\n",
        "            for col in range(3):\n",
        "                # Get the value in the current cell of the game grid\n",
        "                square = self.game_grid[row, col]\n",
        "                # Convert the numerical value to the corresponding mark ('-', 'X', 'O')\n",
        "                grid_drawing[row, col] = mark_labels[square]\n",
        "\n",
        "        # Print the rendered game grid\n",
        "        for row in grid_drawing:\n",
        "            print(' '.join(row))\n",
        "        print('******\\n')\n",
        "\n",
        "    def check_game_result(self):\n",
        "        # Check rows, columns, and diagonals for a win condition\n",
        "        for i in range(3):\n",
        "            # Check rows\n",
        "            if self.game_grid[i, 0] == self.game_grid[i, 1] == self.game_grid[i, 2] and self.game_grid[i, 0] != 0:\n",
        "                return self.game_grid[i, 0]\n",
        "            # Check columns\n",
        "            if self.game_grid[0, i] == self.game_grid[1, i] == self.game_grid[2, i] and self.game_grid[0, i] != 0:\n",
        "                return self.game_grid[0, i]\n",
        "\n",
        "        # Check diagonals\n",
        "        if self.game_grid[0, 0] == self.game_grid[1, 1] == self.game_grid[2, 2] and self.game_grid[0, 0] != 0:\n",
        "            return self.game_grid[0, 0]\n",
        "        if self.game_grid[0, 2] == self.game_grid[1, 1] == self.game_grid[2, 0] and self.game_grid[0, 2] != 0:\n",
        "            return self.game_grid[0, 2]\n",
        "\n",
        "        # Check for draw (grid is full)\n",
        "        if not np.any(self.game_grid == 0):\n",
        "            return 3  # Draw\n",
        "\n",
        "        # Game ongoing\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "hdBbBHDsP-vO"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = AgentVSHumanEnv()"
      ],
      "metadata": {
        "id": "7BTy5fejQhMC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the environment\n",
        "state = env.reset()\n",
        "done = False\n",
        "score = 0\n",
        "\n",
        "# Choose player symbol (X or O)\n",
        "player_symbol = input(\"Choose your symbol:\\n1 for X\\n2 for O\\n\").upper()\n",
        "\n",
        "# Determine the agent's symbol\n",
        "agent_symbol = '1' if player_symbol == '2' else '2'\n",
        "\n",
        "if player_symbol == '1':\n",
        "  env.render()\n",
        "\n",
        "# Game loop\n",
        "while not done:\n",
        "  # Your turn (if applicable)\n",
        "  if env.current_player == int(player_symbol):  # Your turn\n",
        "    print(f\"Your turn! ({mark_labels[env.current_player]})\")\n",
        "    # Allow the palyer to choose only valid input\n",
        "    while True:\n",
        "      player_selected_square = int(input(\"\\nChoose Square (0-8): \").upper()) # (0-8)\n",
        "      if player_selected_square < 0 or player_selected_square > 8:\n",
        "        print(\"Invalid Square number. Valid squares are 0-8. try another square!\")\n",
        "        continue\n",
        "\n",
        "      row, col = divmod(player_selected_square, 3)\n",
        "      # try:\n",
        "      if env.game_grid[row][col] == 0:\n",
        "        break\n",
        "      else:\n",
        "        print(\"Square already marked, try another square!\")\n",
        "\n",
        "    env.game_grid[row][col] = player_symbol\n",
        "    env.render()\n",
        "    result = env.check_game_result()\n",
        "    # Check if turn ended in a draw/win\n",
        "    if result == 3:\n",
        "      done = True\n",
        "      break\n",
        "    elif result != 0:\n",
        "      winner = int(player_symbol)\n",
        "      done = True\n",
        "      break\n",
        "\n",
        "    # Change player to agent\n",
        "    env.current_player = int(agent_symbol)\n",
        "\n",
        "\n",
        "  # Agent's turn\n",
        "  else:\n",
        "    print(\"Agent's turn!\")\n",
        "    action = dqn.forward(state)\n",
        "\n",
        "    # Perform the action\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    score += reward\n",
        "    state = next_state\n",
        "    env.render()\n",
        "    result = env.check_game_result()\n",
        "    if result == 3:\n",
        "      done = True\n",
        "      break\n",
        "    elif result != 0:\n",
        "      winner = int(agent_symbol)\n",
        "      done = True\n",
        "      break\n",
        "\n",
        "if env.check_game_result() == 3:\n",
        "  print(f\"\\nGame ended in a Draw!\")\n",
        "else:\n",
        "  print(f\"\\nGame ended in a Victory for {mark_labels[winner]}!\")\n",
        "  if winner == int(player_symbol):\n",
        "    print(\"You won against the AI!\")\n",
        "  else:\n",
        "    print(\"You lost against the AI!\")\n",
        "\n",
        "print(f'\\nAI Score: {score}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmYpy8HNPq-B",
        "outputId": "0c6b75ac-90fa-40a7-f540-850dc0e2fb23"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose your symbol:\n",
            "1 for X\n",
            "2 for O\n",
            "1\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- - -\n",
            "- - -\n",
            "- - -\n",
            "******\n",
            "\n",
            "Your turn! (X)\n",
            "\n",
            "Choose Square (0-8): 4\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- - -\n",
            "- X -\n",
            "- - -\n",
            "******\n",
            "\n",
            "Agent's turn!\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- O -\n",
            "- X -\n",
            "- - -\n",
            "******\n",
            "\n",
            "Your turn! (X)\n",
            "\n",
            "Choose Square (0-8): 8\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- O -\n",
            "- X -\n",
            "- - X\n",
            "******\n",
            "\n",
            "Agent's turn!\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- O -\n",
            "- X -\n",
            "- - X\n",
            "******\n",
            "\n",
            "Agent's turn!\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- O O\n",
            "- X -\n",
            "- - X\n",
            "******\n",
            "\n",
            "Your turn! (X)\n",
            "\n",
            "Choose Square (0-8): 5\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- O O\n",
            "- X X\n",
            "- - X\n",
            "******\n",
            "\n",
            "Agent's turn!\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "- O O\n",
            "O X X\n",
            "- - X\n",
            "******\n",
            "\n",
            "Your turn! (X)\n",
            "\n",
            "Choose Square (0-8): 0\n",
            "\n",
            "Grid state:\n",
            "******\n",
            "X O O\n",
            "O X X\n",
            "- - X\n",
            "******\n",
            "\n",
            "\n",
            "Game ended in a Victory for X!\n",
            "You won against the AI!\n",
            "\n",
            "AI Score: -10\n",
            "\n"
          ]
        }
      ]
    }
  ]
}